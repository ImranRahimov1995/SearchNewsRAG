services:
  redis:
    image: redis:7-alpine
    container_name: searchnewsrag-redis
    restart: unless-stopped
    command: redis-server --appendonly yes
    volumes:
      - ./redis_data:/data
    expose:
      - "6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - searchnewsrag-network
      - monitoring
    labels:
      - "app=redis"
      - "container_name=searchnewsrag-redis"
      - "env=${ENVIRONMENT:-local}"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  chromadb:
    image: chromadb/chroma:latest
    container_name: searchnewsrag-chromadb
    restart: unless-stopped
    volumes:
      - ./chroma_db:/data
    environment:
      IS_PERSISTENT: "TRUE"
      ANONYMIZED_TELEMETRY: "FALSE"
      ALLOW_RESET: "TRUE"
    networks:
      - searchnewsrag-network
      - monitoring
    labels:
      - "app=chromadb"
      - "container_name=searchnewsrag-chromadb"
      - "env=${ENVIRONMENT:-local}"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  postgres:
    image: postgres:16
    container_name: searchnewsrag-postgres
    restart: unless-stopped
    env_file:
      - .env
    volumes:
      - ./pgdata:/var/lib/postgresql/data
    expose:
      - "${POSTGRES_PORT:-5432}"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER:-newsapp}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - searchnewsrag-network
      - monitoring
    labels:
      - "app=postgres"
      - "container_name=searchnewsrag-postgres"
      - "env=${ENVIRONMENT:-local}"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: searchnewsrag-backend
    restart: unless-stopped
    env_file: .env
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    volumes:
      - ./datanew:/app/datanew
      - ./chroma_db:/app/chroma_db
    depends_on:
      - chromadb
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    command: ["/app/.venv/bin/python", "src/main.py"]
    networks:
      - searchnewsrag-network
      - monitoring
    labels:
      - "app=backend"
      - "container_name=searchnewsrag-backend"
      - "env=${ENVIRONMENT:-local}"
      - "service=searchnewsrag-api"
      - "logging.enabled=true"
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=8000"
      - "prometheus.io/path=/metrics"

  celery-worker:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: searchnewsrag-celery-worker
    restart: unless-stopped
    env_file: .env
    command: sh -c "cd src && celery -A tasks.celery_app worker --loglevel=info --concurrency=${CELERY_WORKER_CONCURRENCY:-2}"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      chromadb:
        condition: service_started
    volumes:
      - ./datanew:/app/datanew
    deploy:
      resources:
        limits:
          cpus: '${CELERY_WORKER_CPU_LIMIT:-2}'
          memory: ${CELERY_WORKER_MEMORY_LIMIT:-2G}
        reservations:
          cpus: '${CELERY_WORKER_CPU_RESERVATION:-0.5}'
          memory: ${CELERY_WORKER_MEMORY_RESERVATION:-512M}
    networks:
      - searchnewsrag-network
      - monitoring
    labels:
      - "app=celery-worker"
      - "container_name=searchnewsrag-celery-worker"
      - "env=${ENVIRONMENT:-local}"
      - "service=background-worker"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  celery-beat:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: searchnewsrag-celery-beat
    restart: unless-stopped
    env_file: .env
    command: sh -c "cd src && celery -A tasks.celery_app beat --loglevel=info --scheduler=celery.beat:PersistentScheduler"
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./celerybeat-schedule:/app/celerybeat-schedule
    deploy:
      resources:
        limits:
          cpus: '${CELERY_BEAT_CPU_LIMIT:-0.5}'
          memory: ${CELERY_BEAT_MEMORY_LIMIT:-256M}
        reservations:
          cpus: '${CELERY_BEAT_CPU_RESERVATION:-0.1}'
          memory: ${CELERY_BEAT_MEMORY_RESERVATION:-128M}
    networks:
      - searchnewsrag-network
      - monitoring
    labels:
      - "app=celery-beat"
      - "container_name=searchnewsrag-celery-beat"
      - "env=${ENVIRONMENT:-local}"
      - "service=task-scheduler"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  flower:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: searchnewsrag-flower
    restart: unless-stopped
    env_file: .env
    command: sh -c "cd src && celery -A tasks.celery_app flower --port=5555 --basic_auth=${FLOWER_USER}:${FLOWER_PASSWORD}"
    ports:
      - "${FLOWER_PORT:-5555}:5555"
    depends_on:
      redis:
        condition: service_healthy
      celery-worker:
        condition: service_started
    deploy:
      resources:
        limits:
          cpus: '${FLOWER_CPU_LIMIT:-0.5}'
          memory: ${FLOWER_MEMORY_LIMIT:-256M}
        reservations:
          cpus: '${FLOWER_CPU_RESERVATION:-0.1}'
          memory: ${FLOWER_MEMORY_RESERVATION:-128M}
    networks:
      - searchnewsrag-network
      - monitoring
    labels:
      - "app=flower"
      - "container_name=searchnewsrag-flower"
      - "env=${ENVIRONMENT:-local}"
      - "service=celery-monitoring"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        VITE_API_BASE_URL: /api
    container_name: searchnewsrag-frontend
    restart: unless-stopped
    expose:
      - "80"
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - searchnewsrag-network
      - monitoring
    labels:
      - "app=frontend"
      - "container_name=searchnewsrag-frontend"
      - "env=${ENVIRONMENT:-local}"
      - "service=searchnewsrag-web"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  nginx:
    build:
      context: ./docker/nginx
      dockerfile: Dockerfile
    container_name: searchnewsrag-nginx
    restart: unless-stopped
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
    depends_on:
      - backend
      - frontend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - searchnewsrag-network
      - monitoring
    labels:
      - "app=nginx"
      - "container_name=searchnewsrag-nginx"
      - "env=${ENVIRONMENT:-local}"
      - "service=reverse-proxy"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

networks:
  searchnewsrag-network:
    driver: bridge
  monitoring:
    external: true
    name: ${MONITORING_NETWORK_NAME:-monitoring}
