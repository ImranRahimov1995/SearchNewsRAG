version: '3.8'

services:
  redis:
    image: redis:7-alpine
    container_name: searchnewsrag-redis-prod
    restart: unless-stopped
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    expose:
      - "6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - searchnewsrag-network
    labels:
      - "app=redis"
      - "container_name=searchnewsrag-redis-prod"
      - "env=production"
      - "service=cache"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  postgres:
    image: postgres:16-alpine
    container_name: searchnewsrag-postgres-prod
    restart: unless-stopped
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=en_US.UTF-8"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - searchnewsrag-network
    labels:
      - "app=postgres"
      - "container_name=searchnewsrag-postgres-prod"
      - "env=production"
      - "service=database"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  chromadb:
    image: chromadb/chroma:latest
    container_name: searchnewsrag-chromadb
    restart: unless-stopped
    volumes:
      - chroma_data:/data
    environment:
      IS_PERSISTENT: "TRUE"
      ANONYMIZED_TELEMETRY: "FALSE"
      ALLOW_RESET: "FALSE"
    networks:
      - searchnewsrag-network
    labels:
      - "app=chromadb"
      - "container_name=searchnewsrag-chromadb-prod"
      - "env=production"
      - "service=vector-database"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  backend:
    image: ${DOCKERHUB_USERNAME}/searchnewsrag-backend:latest
    container_name: searchnewsrag-backend
    restart: unless-stopped
    environment:
      SETTINGS: ${SETTINGS}
      ENVIRONMENT: ${ENVIRONMENT}
      HOST: ${HOST}
      PORT: ${PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      LOG_FORMAT: ${LOG_FORMAT}

      OPENAI_API_KEY: ${OPENAI_API_KEY}

      API_ID: ${TELEGRAM_API_ID}
      API_HASH: ${TELEGRAM_API_HASH}

      DATABASE_URL: ${DATABASE_URL}
      ASYNC_DATABASE_URL: ${ASYNC_DATABASE_URL}
      CHROMA_COLLECTION_NAME: ${CHROMA_COLLECTION_NAME}
      CHROMA_HOST: ${CHROMA_HOST}
      CHROMA_PORT: ${CHROMA_PORT}
      CHROMA_DB_PATH: ${CHROMA_DB_PATH}

      REDIS_URL: ${REDIS_URL}

      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      JWT_ALGORITHM: ${JWT_ALGORITHM}
      JWT_ACCESS_TOKEN_EXPIRE_MINUTES: ${JWT_ACCESS_TOKEN_EXPIRE_MINUTES}
      JWT_REFRESH_TOKEN_EXPIRE_DAYS: ${JWT_REFRESH_TOKEN_EXPIRE_DAYS}

      EMAIL_HOST: ${EMAIL_HOST}
      EMAIL_PORT: ${EMAIL_PORT}
      EMAIL_USE_TLS: ${EMAIL_USE_TLS}
      EMAIL_HOST_USER: ${EMAIL_HOST_USER}
      EMAIL_HOST_PASSWORD: ${EMAIL_HOST_PASSWORD}
      DEFAULT_FROM_EMAIL: ${DEFAULT_FROM_EMAIL}

      OTP_LENGTH: ${OTP_LENGTH}
      OTP_EXPIRE_MINUTES: ${OTP_EXPIRE_MINUTES}
      OTP_MAX_ATTEMPTS: ${OTP_MAX_ATTEMPTS}

      SUPERUSER_EMAIL: ${SUPERUSER_EMAIL}
      SUPERUSER_PASSWORD: ${SUPERUSER_PASSWORD}

      CORS_ORIGINS: ${CORS_ORIGINS}
    command: ["/app/.venv/bin/python", "src/main.py"]
    expose:
      - "8000"
    depends_on:
      postgres:
        condition: service_healthy
      chromadb:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - searchnewsrag-network
    labels:
      - "app=backend"
      - "container_name=searchnewsrag-backend-prod"
      - "env=production"
      - "service=searchnewsrag-api"
      - "logging.enabled=true"
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=8000"
      - "prometheus.io/path=/metrics"

  celery-worker:
    image: ${DOCKERHUB_USERNAME}/searchnewsrag-backend:latest
    container_name: searchnewsrag-celery-worker-prod
    restart: unless-stopped
    command: sh -c "cd src && celery -A tasks.celery_app worker --loglevel=info --concurrency=${CELERY_WORKER_CONCURRENCY}"
    environment:
      SETTINGS: ${SETTINGS}
      ENVIRONMENT: ${ENVIRONMENT}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      API_ID: ${TELEGRAM_API_ID}
      API_HASH: ${TELEGRAM_API_HASH}
      TELEGRAM_SESSION_PATH: ${TELEGRAM_SESSION_PATH}
      DATABASE_URL: ${DATABASE_URL}
      ASYNC_DATABASE_URL: ${ASYNC_DATABASE_URL}
      CHROMA_COLLECTION_NAME: ${CHROMA_COLLECTION_NAME}
      CHROMA_HOST: ${CHROMA_HOST}
      CHROMA_PORT: ${CHROMA_PORT}
      CHROMA_DB_PATH: ${CHROMA_DB_PATH}
      REDIS_URL: ${REDIS_URL}
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      chromadb:
        condition: service_started
    volumes:
      - datanew_data:/app/datanew
      - session_data:/app/session.session
    deploy:
      resources:
        limits:
          cpus: '${CELERY_WORKER_CPU_LIMIT:-4}'
          memory: ${CELERY_WORKER_MEMORY_LIMIT:-4G}
        reservations:
          cpus: '${CELERY_WORKER_CPU_RESERVATION:-1}'
          memory: ${CELERY_WORKER_MEMORY_RESERVATION:-1G}
    networks:
      - searchnewsrag-network
    labels:
      - "app=celery-worker"
      - "container_name=searchnewsrag-celery-worker-prod"
      - "env=production"
      - "service=background-worker"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  celery-beat:
    image: ${DOCKERHUB_USERNAME}/searchnewsrag-backend:latest
    container_name: searchnewsrag-celery-beat-prod
    restart: unless-stopped
    command: sh -c "cd src && celery -A tasks.celery_app beat --loglevel=info --scheduler=celery.beat:PersistentScheduler"
    environment:
      SETTINGS: ${SETTINGS}
      ENVIRONMENT: ${ENVIRONMENT}
      REDIS_URL: ${REDIS_URL}
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - celerybeat_data:/app/celerybeat-schedule
    deploy:
      resources:
        limits:
          cpus: '${CELERY_BEAT_CPU_LIMIT:-0.5}'
          memory: ${CELERY_BEAT_MEMORY_LIMIT:-256M}
        reservations:
          cpus: '${CELERY_BEAT_CPU_RESERVATION:-0.1}'
          memory: ${CELERY_BEAT_MEMORY_RESERVATION:-128M}
    networks:
      - searchnewsrag-network
    labels:
      - "app=celery-beat"
      - "container_name=searchnewsrag-celery-beat-prod"
      - "env=production"
      - "service=task-scheduler"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  flower:
    image: ${DOCKERHUB_USERNAME}/searchnewsrag-backend:latest
    container_name: searchnewsrag-flower-prod
    restart: unless-stopped
    command: sh -c "cd src && celery -A tasks.celery_app flower --port=5555 --basic_auth=${FLOWER_USER}:${FLOWER_PASSWORD}"
    environment:
      SETTINGS: ${SETTINGS}
      ENVIRONMENT: ${ENVIRONMENT}
      REDIS_URL: ${REDIS_URL}
    expose:
      - "5555"
    depends_on:
      redis:
        condition: service_healthy
      celery-worker:
        condition: service_started
    deploy:
      resources:
        limits:
          cpus: '${FLOWER_CPU_LIMIT:-0.5}'
          memory: ${FLOWER_MEMORY_LIMIT:-256M}
        reservations:
          cpus: '${FLOWER_CPU_RESERVATION:-0.1}'
          memory: ${FLOWER_MEMORY_RESERVATION:-128M}
    networks:
      - searchnewsrag-network
    labels:
      - "app=flower"
      - "container_name=searchnewsrag-flower-prod"
      - "env=production"
      - "service=celery-monitoring"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  frontend:
    image: ${DOCKERHUB_USERNAME}/searchnewsrag-frontend:latest
    container_name: searchnewsrag-frontend
    restart: unless-stopped
    expose:
      - "80"
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - searchnewsrag-network
    labels:
      - "app=frontend"
      - "container_name=searchnewsrag-frontend-prod"
      - "env=production"
      - "service=searchnewsrag-web"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

  nginx:
    image: ${DOCKERHUB_USERNAME}/searchnewsrag-nginx:latest
    container_name: searchnewsrag-nginx
    restart: unless-stopped
    ports:
      - "8080:80"
      - "8443:443"
    depends_on:
      - backend
      - frontend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - searchnewsrag-network
    labels:
      - "app=nginx"
      - "container_name=searchnewsrag-nginx-prod"
      - "env=production"
      - "service=reverse-proxy"
      - "logging.enabled=true"
      - "prometheus.io/scrape=false"

networks:
  searchnewsrag-network:
    driver: bridge

volumes:
  redis_data:
    driver: local
  postgres_data:
    driver: local
  chroma_data:
    driver: local
  datanew_data:
    driver: local
  celerybeat_data:
    driver: local
  session_data:
    driver: local
